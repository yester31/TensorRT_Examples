# by yhpark 2025-9-27
import numpy as np
import torch
import torch.nn as nn
import cv2
import os
import sys

sys.path.insert(1, os.path.join(sys.path[0], "RT-DETR/rtdetrv2_pytorch"))
from src.core import YAMLConfig

CUR_DIR = os.path.dirname(os.path.abspath(__file__))
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

def transform_cv(image):    
    # 0) BGR -> RGB (필요 시 주석 해제)
    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # 1) Resize (384,384)
    image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_LINEAR)

    # 2) ToTensor (HWC -> CHW, 0~1 float)
    image = image.astype(np.float32) / 255.0
    image = np.transpose(image, (2, 0, 1))  # (H,W,C) -> (C,H,W)

    # 3) Add batch dimension
    image = np.expand_dims(image, axis=0)  # (1,C,H,W)

    # Return as NumPy array (C-order)   
    return np.array(image, dtype=np.float32, order="C")

_COLORS = np.array(
    [
        0.000, 0.447, 0.741,
        0.850, 0.325, 0.098,
        0.929, 0.694, 0.125,
        0.494, 0.184, 0.556,
        0.466, 0.674, 0.188,
        0.301, 0.745, 0.933,
        0.635, 0.078, 0.184,
        0.300, 0.300, 0.300,
        0.600, 0.600, 0.600,
        1.000, 0.000, 0.000,
        1.000, 0.500, 0.000,
        0.749, 0.749, 0.000,
        0.000, 1.000, 0.000,
        0.000, 0.000, 1.000,
        0.667, 0.000, 1.000,
        0.333, 0.333, 0.000,
        0.333, 0.667, 0.000,
        0.333, 1.000, 0.000,
        0.667, 0.333, 0.000,
        0.667, 0.667, 0.000,
        0.667, 1.000, 0.000,
        1.000, 0.333, 0.000,
        1.000, 0.667, 0.000,
        1.000, 1.000, 0.000,
        0.000, 0.333, 0.500,
        0.000, 0.667, 0.500,
        0.000, 1.000, 0.500,
        0.333, 0.000, 0.500,
        0.333, 0.333, 0.500,
        0.333, 0.667, 0.500,
        0.333, 1.000, 0.500,
        0.667, 0.000, 0.500,
        0.667, 0.333, 0.500,
        0.667, 0.667, 0.500,
        0.667, 1.000, 0.500,
        1.000, 0.000, 0.500,
        1.000, 0.333, 0.500,
        1.000, 0.667, 0.500,
        1.000, 1.000, 0.500,
        0.000, 0.333, 1.000,
        0.000, 0.667, 1.000,
        0.000, 1.000, 1.000,
        0.333, 0.000, 1.000,
        0.333, 0.333, 1.000,
        0.333, 0.667, 1.000,
        0.333, 1.000, 1.000,
        0.667, 0.000, 1.000,
        0.667, 0.333, 1.000,
        0.667, 0.667, 1.000,
        0.667, 1.000, 1.000,
        1.000, 0.000, 1.000,
        1.000, 0.333, 1.000,
        1.000, 0.667, 1.000,
        0.333, 0.000, 0.000,
        0.500, 0.000, 0.000,
        0.667, 0.000, 0.000,
        0.833, 0.000, 0.000,
        1.000, 0.000, 0.000,
        0.000, 0.167, 0.000,
        0.000, 0.333, 0.000,
        0.000, 0.500, 0.000,
        0.000, 0.667, 0.000,
        0.000, 0.833, 0.000,
        0.000, 1.000, 0.000,
        0.000, 0.000, 0.167,
        0.000, 0.000, 0.333,
        0.000, 0.000, 0.500,
        0.000, 0.000, 0.667,
        0.000, 0.000, 0.833,
        0.000, 0.000, 1.000,
        0.000, 0.000, 0.000,
        0.143, 0.143, 0.143,
        0.286, 0.286, 0.286,
        0.429, 0.429, 0.429,
        0.571, 0.571, 0.571,
        0.714, 0.714, 0.714,
        0.857, 0.857, 0.857,
        0.000, 0.447, 0.741,
        0.314, 0.717, 0.741,
        0.50, 0.5, 0
    ]
).astype(np.float32).reshape(-1, 3)

COCO_CLASSES = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
    'potted plant', 'bed', 'dining table', 'toilet', 'TV', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
    'hair drier', 'toothbrush']

def draw_bounding_boxes(image, boxes, labels, scores, thrh = 0.6):
    """
    Draw bounding boxes with labels and scores on an image.
    
    Args:
        image (np.ndarray): Input BGR image (H, W, 3)
        boxes (np.ndarray): Bounding boxes [N, 4] in (x1, y1, x2, y2)
        labels (list[int] or np.ndarray): Class indices for each box
        scores (list[float] or np.ndarray): Confidence scores for each box
    Returns:
        np.ndarray: Image with drawn boxes
    """

    # === Precompute frequently used lookups ===
    # Convert colors to uint8 only once
    colors = (_COLORS[labels] * 255).astype(np.uint8)  

    for i, box in enumerate(boxes):
        # Extract box coordinates (cast to int once)
        x1, y1, x2, y2 = map(int, box)

        # Fetch label and score
        label = labels[i]
        score = scores[i]

        # Pick precomputed color
        color = tuple(int(c) for c in colors[i])  

        # Draw rectangle (thin outline for speed)
        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness=1)

        # Build text string only once
        text = f"{label} {score:.2f} {COCO_CLASSES[label]}"
        print(text)
        # Draw label text above bounding box
        cv2.putText(
            image,
            text,
            (x1, max(y1 - 5, 0)),  # Ensure text not out of bounds
            fontFace=cv2.FONT_HERSHEY_SIMPLEX,
            fontScale=0.5,
            color=color,
            thickness=1,
            lineType=cv2.LINE_AA
        )

    return image

class Model(nn.Module):
    def __init__(self, cfg) -> None:
        super().__init__()
        self.model = cfg.model.deploy()
        self.postprocessor = cfg.postprocessor.deploy()
        
    def forward(self, images, orig_target_sizes):
        outputs = self.model(images)
        outputs = self.postprocessor(outputs, orig_target_sizes)
        return outputs

def main():

    config = f"{CUR_DIR}/RT-DETR/rtdetrv2_pytorch/configs/rtdetrv2/rtdetrv2_r18vd_120e_coco.yml"
    resume = f"{CUR_DIR}/RT-DETR/rtdetrv2_pytorch/pretrained/rtdetrv2_r18vd_120e_coco_rerun_48.1.pth"
    cfg = YAMLConfig(config, resume=resume)
    if resume:
        checkpoint = torch.load(resume, map_location='cpu') 
        if 'ema' in checkpoint:
            state = checkpoint['ema']['module']
        else:
            state = checkpoint['model']
    else:
        raise AttributeError('Only support resume to load model.state_dict by now.')
    # NOTE load train mode state -> convert to deploy mode
    cfg.model.load_state_dict(state)
    model = Model(cfg)
    model = model.eval().to(DEVICE)

    image_path = f"{CUR_DIR}/data/dog.jpg"
    image_file_name = os.path.splitext(os.path.basename(image_path))[0]
    raw_image = cv2.imread(image_path)
    height, width = raw_image.shape[:2] # [h, w, c]
    print(f"[MDET] original image size : {height, width}")
    image_rgb = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)
    tensor = transform_cv(image_rgb)
    tensor = torch.from_numpy(tensor).to(DEVICE)
    orig_size = torch.tensor([[width, height]]).to(DEVICE)

    with torch.no_grad():
        output = model(tensor, orig_size)

    labels, boxes, scores = output

    labels = labels.detach().cpu().numpy()
    boxes = boxes.detach().cpu().numpy()
    scores = scores.detach().cpu().numpy()

    thrh=0.6
    scr = scores[0]
    lab = labels[0][scr > thrh]
    box = boxes[0][scr > thrh]
    scrs = scr[scr > thrh]

    output_img = draw_bounding_boxes(raw_image, box, lab, scrs)

    save_path = os.path.join(CUR_DIR, 'results', f"{image_file_name}_pt.jpg")
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    cv2.imwrite(save_path, output_img)
    print("Image processing complete.")

if __name__ == '__main__':
    main()

